{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":10809191,"sourceType":"datasetVersion","datasetId":6709804}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetuning using Unsloth for Automated Essay Scoring using Open-Source LLM","metadata":{}},{"cell_type":"markdown","source":"## 1. Install Required Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -q -U unsloth datasets trl peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:37:41.337783Z","iopub.execute_input":"2025-03-10T01:37:41.338079Z","iopub.status.idle":"2025-03-10T01:40:37.513376Z","shell.execute_reply.started":"2025-03-10T01:37:41.338049Z","shell.execute_reply":"2025-03-10T01:40:37.512539Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m693.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.6/191.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Set Up Environment & Imports","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nfrom datasets import Dataset\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error, accuracy_score, confusion_matrix\nfrom scipy.stats import spearmanr\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:40:37.514363Z","iopub.execute_input":"2025-03-10T01:40:37.514692Z","iopub.status.idle":"2025-03-10T01:41:06.626308Z","shell.execute_reply.started":"2025-03-10T01:40:37.514665Z","shell.execute_reply":"2025-03-10T01:41:06.625607Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Load Dataset\n- Reads train, validation, and test datasets from an Excel file.\n- Resets indices to avoid issues when applying functions.","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/cleaned-automated-essay-grader/train_test_data.xlsx\"\ntrain_df = pd.read_excel(data_path, sheet_name=\"Train\")\nval_df = pd.read_excel(data_path, sheet_name=\"Validation\")\ntest_df = pd.read_excel(data_path, sheet_name=\"Test\")\n\n# Reset indices\nX_train, X_eval, X_test = train_df.reset_index(drop=True), val_df.reset_index(drop=True), test_df.reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:06.627120Z","iopub.execute_input":"2025-03-10T01:41:06.627423Z","iopub.status.idle":"2025-03-10T01:41:07.353222Z","shell.execute_reply.started":"2025-03-10T01:41:06.627385Z","shell.execute_reply":"2025-03-10T01:41:07.352561Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"X_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.354731Z","iopub.execute_input":"2025-03-10T01:41:07.355271Z","iopub.status.idle":"2025-03-10T01:41:07.382255Z","shell.execute_reply.started":"2025-03-10T01:41:07.355249Z","shell.execute_reply":"2025-03-10T01:41:07.381443Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"    essay_id                                          full_text  score\n0    678631b  I think that we should not have that in are cl...      1\n1    c6db974  I believe that driverless are something that s...      4\n2    b5962cc  In foreign countries, residents appear to pref...      1\n3    d5f4814  In this article \"making Mona Lisa Smile\", it t...      5\n4    4332440  that people can still steer and accelerte and ...      1\n..       ...                                                ...    ...\n773  31e00b5  \"With less cars we have less accidents and les...      4\n774  efd13e6  Have you ever thought about what life would be...      4\n775  4d00492  Dear Senator,\\n\\nConcerning the topic of the m...      6\n776  178f445  In the 21st century there has\\n\\nbeen a lot of...      5\n777  c5d4de9  Someone takes a selfie, and sends it to their ...      4\n\n[778 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>678631b</td>\n      <td>I think that we should not have that in are cl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c6db974</td>\n      <td>I believe that driverless are something that s...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b5962cc</td>\n      <td>In foreign countries, residents appear to pref...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d5f4814</td>\n      <td>In this article \"making Mona Lisa Smile\", it t...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4332440</td>\n      <td>that people can still steer and accelerte and ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>773</th>\n      <td>31e00b5</td>\n      <td>\"With less cars we have less accidents and les...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>774</th>\n      <td>efd13e6</td>\n      <td>Have you ever thought about what life would be...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>4d00492</td>\n      <td>Dear Senator,\\n\\nConcerning the topic of the m...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>776</th>\n      <td>178f445</td>\n      <td>In the 21st century there has\\n\\nbeen a lot of...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>777</th>\n      <td>c5d4de9</td>\n      <td>Someone takes a selfie, and sends it to their ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>778 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"X_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.383718Z","iopub.execute_input":"2025-03-10T01:41:07.383940Z","iopub.status.idle":"2025-03-10T01:41:07.392666Z","shell.execute_reply.started":"2025-03-10T01:41:07.383920Z","shell.execute_reply":"2025-03-10T01:41:07.392032Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   essay_id                                          full_text  score\n0   21e1fd4  Senator <blank>,\\n\\nI would like to express my...      3\n1   0435f85  The author didnt support this claim very well....      3\n2   7fa6c4e  Luke was 18 years old. He just finished his hi...      1\n3   5ac9060  I am completely against driverless cars. There...      4\n4   ccc4a3b  State senator.....Keeping the Electoral Colleg...      1\n..      ...                                                ...    ...\n92  7044865  Everyday when you wake up and get ready for wo...      5\n93  70bb147  Are Driverless Cars the Future or Death to Man...      3\n94  ce0be14  I think other should participate in the Seagoi...      2\n95  6271110  The Electoral College is a system that must be...      6\n96  7152619  We live in an age of technology where one coul...      3\n\n[97 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21e1fd4</td>\n      <td>Senator &lt;blank&gt;,\\n\\nI would like to express my...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0435f85</td>\n      <td>The author didnt support this claim very well....</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7fa6c4e</td>\n      <td>Luke was 18 years old. He just finished his hi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5ac9060</td>\n      <td>I am completely against driverless cars. There...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ccc4a3b</td>\n      <td>State senator.....Keeping the Electoral Colleg...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>7044865</td>\n      <td>Everyday when you wake up and get ready for wo...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>70bb147</td>\n      <td>Are Driverless Cars the Future or Death to Man...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>ce0be14</td>\n      <td>I think other should participate in the Seagoi...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>6271110</td>\n      <td>The Electoral College is a system that must be...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>7152619</td>\n      <td>We live in an age of technology where one coul...</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>97 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.393433Z","iopub.execute_input":"2025-03-10T01:41:07.393701Z","iopub.status.idle":"2025-03-10T01:41:07.410185Z","shell.execute_reply.started":"2025-03-10T01:41:07.393681Z","shell.execute_reply":"2025-03-10T01:41:07.409359Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   essay_id                                          full_text  score\n0   459922f  cars, they wernt invented in the old days. who...      2\n1   78569ca  Limiting car usage would be a wise thing to do...      2\n2   fad3429  Since World War II we have been heavily depend...      5\n3   1bdcde3  In \"The challenge of the Exploring Venus\" it s...      3\n4   b6994cb  The Electoral College, established by the Foun...      6\n..      ...                                                ...    ...\n93  2aca278  One big milestone in a person's teenage years ...      5\n94  680c26b  Driverless cars are a thing of the future. The...      5\n95  9e1909b  Politicians and the public have argued for yea...      5\n96  98bbdf1  The electoral college is an institution that h...      5\n97  e7ee5ef  I believe that the use of facial Action Coding...      2\n\n[98 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>459922f</td>\n      <td>cars, they wernt invented in the old days. who...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>78569ca</td>\n      <td>Limiting car usage would be a wise thing to do...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fad3429</td>\n      <td>Since World War II we have been heavily depend...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1bdcde3</td>\n      <td>In \"The challenge of the Exploring Venus\" it s...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b6994cb</td>\n      <td>The Electoral College, established by the Foun...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>2aca278</td>\n      <td>One big milestone in a person's teenage years ...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>680c26b</td>\n      <td>Driverless cars are a thing of the future. The...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>9e1909b</td>\n      <td>Politicians and the public have argued for yea...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>98bbdf1</td>\n      <td>The electoral college is an institution that h...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>e7ee5ef</td>\n      <td>I believe that the use of facial Action Coding...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>98 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## 4. Generate Prompts for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"def generate_prompt(data_point):\n    return f\"\"\"\n            Evaluate the following essay using a **1 to 6** scoring scale.\n\n            **Scoring Criteria:**\n            - **6** = Excellent: Insightful, well-structured, strong evidence, minimal errors.\n            - **5** = Good: Clear, logical, solid evidence, mostly error-free.\n            - **4** = Satisfactory: Adequate reasoning, relevant examples, minor issues.\n            - **3** = Developing: Some reasoning, weak structure, noticeable errors.\n            - **2** = Poor: Vague argument, poor organization, frequent errors.\n            - **1** = Very weak: No clear argument, incoherent, major issues.\n\n            **Essay:**\n            \"{data_point['full_text']}\"\n\n            **Assign a final score (1 to 6) based on the criteria above.**\n            - **Return the number first then the explanation.**\n\n            **Assigned Score:** {data_point['score']}\n            \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Evaluate the following essay using a **1 to 6** scoring scale.\n\n            **Scoring Criteria:**\n            - **6** = Excellent: Insightful, well-structured, strong evidence, minimal errors.\n            - **5** = Good: Clear, logical, solid evidence, mostly error-free.\n            - **4** = Satisfactory: Adequate reasoning, relevant examples, minor issues.\n            - **3** = Developing: Some reasoning, weak structure, noticeable errors.\n            - **2** = Poor: Vague argument, poor organization, frequent errors.\n            - **1** = Very weak: No clear argument, incoherent, major issues.\n\n            **Essay:**\n            \"{data_point['full_text']}\"\n\n            **Assign a final score (1 to 6) based on the criteria above.**\n            - **Return the number first then the explanation.**\n\n            **Final Score:** \n            \"\"\".strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.411088Z","iopub.execute_input":"2025-03-10T01:41:07.411369Z","iopub.status.idle":"2025-03-10T01:41:07.424013Z","shell.execute_reply.started":"2025-03-10T01:41:07.411335Z","shell.execute_reply":"2025-03-10T01:41:07.423347Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"For test set, use `generate_test_prompt()` for `X_test` to avoid leaking the true score into the prompt. This ensures that the model is making predictions without being influenced by the actual labels.","metadata":{}},{"cell_type":"code","source":"# Apply the prompt generation functions\nX_train['prompt'] = X_train.apply(generate_prompt, axis=1)\nX_eval['prompt'] = X_eval.apply(generate_prompt, axis=1)\nX_test['prompt'] = X_test.apply(generate_test_prompt, axis=1)\n\n# Only keep the prompt column\ntrain_data = Dataset.from_pandas(X_train[['prompt']])\neval_data = Dataset.from_pandas(X_eval[['prompt']])\ntest_data = Dataset.from_pandas(X_test[['prompt']])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.424858Z","iopub.execute_input":"2025-03-10T01:41:07.425151Z","iopub.status.idle":"2025-03-10T01:41:07.503797Z","shell.execute_reply.started":"2025-03-10T01:41:07.425123Z","shell.execute_reply":"2025-03-10T01:41:07.502974Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.504662Z","iopub.execute_input":"2025-03-10T01:41:07.504889Z","iopub.status.idle":"2025-03-10T01:41:07.511805Z","shell.execute_reply.started":"2025-03-10T01:41:07.504869Z","shell.execute_reply":"2025-03-10T01:41:07.511121Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'prompt': 'Evaluate the following essay using a **1 to 6** scoring scale.\\n\\n            **Scoring Criteria:**\\n            - **6** = Excellent: Insightful, well-structured, strong evidence, minimal errors.\\n            - **5** = Good: Clear, logical, solid evidence, mostly error-free.\\n            - **4** = Satisfactory: Adequate reasoning, relevant examples, minor issues.\\n            - **3** = Developing: Some reasoning, weak structure, noticeable errors.\\n            - **2** = Poor: Vague argument, poor organization, frequent errors.\\n            - **1** = Very weak: No clear argument, incoherent, major issues.\\n\\n            **Essay:**\\n            \"I think that we should not have that in are classes because that would make people fill unhappy to come to school. Then that wouldamke people not want to come to school because the teachers would know how we are filling today and whats on our minds. Then they would know what we are thinking. Then they would know when we are happy, when we are mad, when we are sad , when we just dont care anymore. Like they said in the first line she? 83 percent happy, 9 ercent disgusted, 6 percent fearful, and 2 percent angry. Then they said that? not your science teacher, grading your latest lab assignment. They start to talk about how many major muscles in the modle must move like human muscles. there are 44 major mucles that must move like human mucles. Thats just in the face. The movement of one or more mucles is called an ¨action unit.¨\\n\\nThey tell you to do this one fun lab thing you can do while you are looking in the mirror at home.\\n\\nIt gives you step by step how to do the lab.   \"\\n\\n            **Assign a final score (1 to 6) based on the criteria above.**\\n            - **Return the number first then the explanation.**\\n\\n            **Assigned Score:** 1'}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"test_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.512522Z","iopub.execute_input":"2025-03-10T01:41:07.512770Z","iopub.status.idle":"2025-03-10T01:41:07.527062Z","shell.execute_reply.started":"2025-03-10T01:41:07.512739Z","shell.execute_reply":"2025-03-10T01:41:07.526455Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'prompt': 'Evaluate the following essay using a **1 to 6** scoring scale.\\n\\n            **Scoring Criteria:**\\n            - **6** = Excellent: Insightful, well-structured, strong evidence, minimal errors.\\n            - **5** = Good: Clear, logical, solid evidence, mostly error-free.\\n            - **4** = Satisfactory: Adequate reasoning, relevant examples, minor issues.\\n            - **3** = Developing: Some reasoning, weak structure, noticeable errors.\\n            - **2** = Poor: Vague argument, poor organization, frequent errors.\\n            - **1** = Very weak: No clear argument, incoherent, major issues.\\n\\n            **Essay:**\\n            \"cars, they wernt invented in the old days. who needs them, not me. screw cars. Ill tell u what u need to know about life and transportation. We can limit car use if u just use this information and use it in your life.\\n\\nparis has so much smog. Ew!!! No one likes that so lets limit car use. paris has 147 micrograms of matter in cubic meter measurement. Dang taxes! I hate soccer moms, my mom is one so i cant hate them too much. All they do is drive everywhere and use up all the resources so there kid can strike out in little leauge! Now i will tell you how to fix this problem.\\n\\nif some of the stupid soccer moms driving around there rolling egg (mini van) the car use would be cut in half. Then you might say then what is my little kid going to do. You can read a biycle places just like it said in line 24 of \"car-free day spinning into a big hit bogota. Now ill tell you what kind of bike and what your gonna do with it and your gonna love it!\\n\\nA bmx bike is what im talking about. you can transportate it to the skatepark to ride the ramps. Its really fun and i love it. Those gas using Mini vans would be off the streets and bmx bikes can take over. Think about our earth for once please.\\n\\nwell to conclude things up car use is oversused highly. It is mainly the soccer moms fault. To fix all the problems in the world everyone needs to get a bmx bike and go ride some dirt jumps and this world would be a happier place!!!    \"\\n\\n            **Assign a final score (1 to 6) based on the criteria above.**\\n            - **Return the number first then the explanation.**\\n\\n            **Final Score:**'}"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## 5. Define Evaluation Metrics\n- Implements Quadratic Weighted Kappa (QWK) and Spearman’s Rank Correlation to measure model agreement with human graders.\n- Prints a confusion matrix for error analysis.","metadata":{}},{"cell_type":"code","source":"def compute_metrics(y_true, y_pred):\n    \"\"\"\n    Computes evaluation metrics: QWK, Spearman’s Rank Correlation, MSE, Accuracy, and Soft Accuracy.\n\n    Args:\n        y_true (list): True labels.\n        y_pred (list): Predicted labels.\n\n    Returns:\n        dict: Dictionary of metric results.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Compute Soft Accuracy: proportion of predictions within ±1 of the true label\n    soft_acc = np.mean(np.abs(y_true - y_pred) <= 1)\n\n    metrics = {\n        \"QWK\": cohen_kappa_score(y_true, y_pred, weights=\"quadratic\"),\n        \"Spearman Correlation\": spearmanr(y_true, y_pred)[0],  # Returns (corr, p-value), retrieve corr\n        \"MSE\": mean_squared_error(y_true, y_pred),\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Soft Accuracy\": soft_acc,\n    }\n\n    return metrics \n\ndef plot_confusion_matrix(y_true, y_pred, labels):\n    \"\"\"\n    Plots the confusion matrix.\n\n    Args:\n        y_true (list): True labels.\n        y_pred (list): Predicted labels.\n        labels (list): Label categories.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    labels = np.array(labels)\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n\ndef evaluate(y_true, y_pred):\n    \"\"\"\n    Evaluate model predictions using QWK, Spearman’s Rank Correlation, Accuracy, Soft Accuracy, \n    and visualize the confusion matrix.\n\n    Args:\n        y_true (list or np.array): Ground truth labels.\n        y_pred (list or np.array): Predicted labels.\n    \"\"\"\n    metrics = compute_metrics(y_true, y_pred)\n\n    print(f'Quadratic Weighted Kappa: {metrics[\"QWK\"]:.3f}')\n    print(f\"Spearman's Rank Correlation: {metrics['Spearman Correlation']:.3f}\")\n    print(f'Accuracy: {metrics[\"Accuracy\"]:.3f}')\n    print(f'Soft Accuracy: {metrics[\"Soft Accuracy\"]:.3f}')\n\n    # Plot confusion matrix\n    plot_confusion_matrix(y_true, y_pred, labels=np.unique(y_true))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.527877Z","iopub.execute_input":"2025-03-10T01:41:07.528123Z","iopub.status.idle":"2025-03-10T01:41:07.541339Z","shell.execute_reply.started":"2025-03-10T01:41:07.528104Z","shell.execute_reply":"2025-03-10T01:41:07.540664Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 6. Load LLM Model with Unsloth","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:19:54.149805Z","iopub.execute_input":"2025-02-28T15:19:54.150225Z","iopub.status.idle":"2025-02-28T15:19:54.156513Z","shell.execute_reply.started":"2025-02-28T15:19:54.150187Z","shell.execute_reply":"2025-02-28T15:19:54.155286Z"}}},{"cell_type":"code","source":"max_seq_length = 2048  \ndtype = None  # Auto-detect\nload_in_4bit = True  # Use 4-bit quantization to reduce memory usage.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\n# Set padding token\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:07.542108Z","iopub.execute_input":"2025-03-10T01:41:07.542307Z","iopub.status.idle":"2025-03-10T01:41:49.074072Z","shell.execute_reply.started":"2025-03-10T01:41:07.542288Z","shell.execute_reply":"2025-03-10T01:41:49.073204Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.9: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu118. CUDA: 6.0. CUDA Toolkit: 11.8. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4e4ddd3cf14a749be6d58f2d296d20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9d5cc9175447fe99aeb3f0515cf94b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38a39f9eb4d94ed7b4bef3df76547637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cc49098bcd4af09036ddfa768fb418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded193b6ec434c7ab06b2328cff81d18"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## 7. Apply LoRA to Model","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=64,  # Suggested: 8, 16, 32, 64, 128\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0.1,  \n    bias=\"none\",\n    # use_gradient_checkpointing=\"unsloth\",  # Optimized for long context\n    random_state=42,\n    use_rslora=False,  # Rank Stabilized LoRA disabled\n    loftq_config=None,  # LoftQ disabled\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:49.077206Z","iopub.execute_input":"2025-03-10T01:41:49.077432Z","iopub.status.idle":"2025-03-10T01:41:54.857159Z","shell.execute_reply.started":"2025-03-10T01:41:49.077409Z","shell.execute_reply":"2025-03-10T01:41:54.855974Z"}},"outputs":[{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2025.3.9 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"prompt\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n\n# Apply tokenization\ntrain_data = train_data.map(tokenize_function, batched=True)\neval_data = eval_data.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:54.858493Z","iopub.execute_input":"2025-03-10T01:41:54.858776Z","iopub.status.idle":"2025-03-10T01:41:56.616082Z","shell.execute_reply.started":"2025-03-10T01:41:54.858740Z","shell.execute_reply":"2025-03-10T01:41:56.615390Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/778 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dead3490f6214ea08da3dec400fc75ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/97 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"139fdb35d84a411bba3549863b93527e"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"output_dir = \"trained_weights\"  # Directory to save model checkpoints\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"trained_weights\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=1,\n    eval_accumulation_steps=4,\n    # gradient_checkpointing=True,\n    optim=\"paged_adamw_32bit\",\n    save_steps=10,\n    save_total_limit=2,\n    logging_steps=1,  # Log training loss at each step\n    logging_dir=\"trained_weights/logs\",\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    group_by_length=False,\n    lr_scheduler_type=\"cosine\",\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    logging_strategy=\"steps\",  # Ensure logs appear per step\n    log_level=\"info\",  # Set logging to show detailed output\n    disable_tqdm=False,  # Force showing progress bar\n    report_to=\"none\",  # Avoid using wandb\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:56.616920Z","iopub.execute_input":"2025-03-10T01:41:56.617248Z","iopub.status.idle":"2025-03-10T01:41:56.647394Z","shell.execute_reply.started":"2025-03-10T01:41:56.617215Z","shell.execute_reply":"2025-03-10T01:41:56.646560Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    dataset_text_field=\"prompt\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Can speed up training for short sequences\n    args=training_arguments,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:56.648352Z","iopub.execute_input":"2025-03-10T01:41:56.648669Z","iopub.status.idle":"2025-03-10T01:41:57.566020Z","shell.execute_reply.started":"2025-03-10T01:41:56.648638Z","shell.execute_reply":"2025-03-10T01:41:57.565347Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing auto half precision backend\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:57.566707Z","iopub.execute_input":"2025-03-10T01:41:57.566903Z","iopub.status.idle":"2025-03-10T01:41:57.572358Z","shell.execute_reply.started":"2025-03-10T01:41:57.566886Z","shell.execute_reply":"2025-03-10T01:41:57.571538Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n5.988 GB of memory reserved.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:41:57.573126Z","iopub.execute_input":"2025-03-10T01:41:57.573333Z","iopub.status.idle":"2025-03-10T05:49:39.048315Z","shell.execute_reply.started":"2025-03-10T01:41:57.573314Z","shell.execute_reply":"2025-03-10T05:49:39.047453Z"}},"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 778 | Num Epochs = 1 | Total steps = 778\nO^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n \"-____-\"     Trainable parameters = 167,772,160/4,708,372,480 (3.56% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='778' max='778' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [778/778 4:07:18, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.477700</td>\n      <td>1.797738</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.217700</td>\n      <td>1.728043</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.952400</td>\n      <td>1.681974</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.363500</td>\n      <td>1.648184</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.316700</td>\n      <td>1.628868</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.015100</td>\n      <td>1.615571</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.363400</td>\n      <td>1.609102</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to trained_weights/checkpoint-10\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nSaving model checkpoint to trained_weights/checkpoint-20\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nSaving model checkpoint to trained_weights/checkpoint-30\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-10] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-40\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-20] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-50\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-30] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-60\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-40] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-70\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-50] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-80\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-60] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-90\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-70] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 97\n  Batch size = 1\nSaving model checkpoint to trained_weights/checkpoint-100\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-80] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-110\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-90] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-120\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-100] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-130\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-110] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-140\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-120] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-150\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-130] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-160\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-140] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-170\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-150] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-180\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-160] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-190\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-170] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 97\n  Batch size = 1\nSaving model checkpoint to trained_weights/checkpoint-200\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-180] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-210\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-190] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-220\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-200] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-230\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-210] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-240\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-220] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-250\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-230] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-260\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-240] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-270\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-250] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-280\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-260] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-290\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-270] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 97\n  Batch size = 1\nSaving model checkpoint to trained_weights/checkpoint-300\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-280] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-310\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-290] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-320\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-300] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-330\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-310] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-340\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-320] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-350\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-330] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-360\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-340] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-370\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-350] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-380\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-360] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-390\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-370] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 97\n  Batch size = 1\nSaving model checkpoint to trained_weights/checkpoint-400\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-380] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-410\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-390] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-420\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-400] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-430\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-410] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-440\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-420] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-450\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-430] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-460\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-440] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-470\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-450] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-480\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-460] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-490\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-470] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 97\n  Batch size = 1\nSaving model checkpoint to trained_weights/checkpoint-500\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-480] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-510\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-490] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-520\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-500] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-530\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-510] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-540\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-520] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-550\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-530] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-560\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-540] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-570\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-550] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-580\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-560] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-590\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-570] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 97\n  Batch size = 1\nSaving model checkpoint to trained_weights/checkpoint-600\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-580] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-610\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-590] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-620\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-600] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-630\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-610] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-640\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-620] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-650\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-630] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-660\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-640] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-670\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-650] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-680\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-660] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-690\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-670] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 97\n  Batch size = 1\nSaving model checkpoint to trained_weights/checkpoint-700\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-680] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-710\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-690] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-720\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-700] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-730\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-710] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-740\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-720] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-750\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-730] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-760\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-740] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-770\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-750] due to args.save_total_limit\nSaving model checkpoint to trained_weights/checkpoint-778\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nDeleting older checkpoint [trained_weights/checkpoint-760] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:49:39.050509Z","iopub.execute_input":"2025-03-10T05:49:39.050796Z","iopub.status.idle":"2025-03-10T05:49:39.057289Z","shell.execute_reply.started":"2025-03-10T05:49:39.050775Z","shell.execute_reply":"2025-03-10T05:49:39.056408Z"}},"outputs":[{"name":"stdout","text":"14859.377 seconds used for training.\n247.66 minutes used for training.\nPeak reserved memory = 9.371 GB.\nPeak reserved memory for training = 3.383 GB.\nPeak reserved memory % of max memory = 58.982 %.\nPeak reserved memory for training % of max memory = 21.293 %.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Save trained model and tokenizer\ntrainer.save_model()\ntokenizer.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:49:39.058161Z","iopub.execute_input":"2025-03-10T05:49:39.058472Z","iopub.status.idle":"2025-03-10T05:49:40.801251Z","shell.execute_reply.started":"2025-03-10T05:49:39.058437Z","shell.execute_reply":"2025-03-10T05:49:40.800225Z"}},"outputs":[{"name":"stderr","text":"Saving model checkpoint to trained_weights\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/f15c379fb32bb402fa06a7ae9aecb1febf4b79ec/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": [\n      \"lm_head\",\n      \"multi_modal_projector\",\n      \"merger\",\n      \"modality_projection\"\n    ],\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"unsloth_fixed\": true,\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('trained_weights/tokenizer_config.json',\n 'trained_weights/special_tokens_map.json',\n 'trained_weights/tokenizer.json')"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"## 7. Predict function","metadata":{}},{"cell_type":"code","source":"import random\nimport \n\ndef predict(X_test, model, tokenizer, seed=42):\n    y_pred = []\n\n    # Set the seed globally for reproducibility\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # Initialize the pipeline without `seed`\n    pipe = pipeline(\n        task=\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=3,  # Allow enough space for model to generate a number\n    )\n\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"prompt\"]\n\n        result = pipe(prompt + \"\\nAssigned Score (1 to 6):\")\n        generated_text = result[0]['generated_text'].strip()\n\n        # Extract the answer using regex to find a standalone number between 1 and 6\n        match = re.findall(r'\\b[1-6]\\b', generated_text)\n        if match:\n            answer = int(match[-1])  # Take the last matched number\n            y_pred.append(answer)\n        else:\n            y_pred.append(None)  # Handling unexpected outputs\n\n    return y_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:55:59.485902Z","iopub.execute_input":"2025-03-10T05:55:59.486267Z","iopub.status.idle":"2025-03-10T05:55:59.492147Z","shell.execute_reply.started":"2025-03-10T05:55:59.486238Z","shell.execute_reply":"2025-03-10T05:55:59.491382Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def predict(X_test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"prompt\"]\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=3)\n\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n        match = re.findall(r'\\b[1-6]\\b', generated_text)\n        y_pred.append(int(match[-1]) if match else None)\n    return y_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:57:41.181416Z","iopub.execute_input":"2025-03-10T05:57:41.181716Z","iopub.status.idle":"2025-03-10T05:57:41.186663Z","shell.execute_reply.started":"2025-03-10T05:57:41.181694Z","shell.execute_reply":"2025-03-10T05:57:41.185899Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Run inference with batch processing\npredicted_scores = predict(X_test, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:57:42.801465Z","iopub.execute_input":"2025-03-10T05:57:42.801794Z","iopub.status.idle":"2025-03-10T06:01:03.123041Z","shell.execute_reply.started":"2025-03-10T05:57:42.801766Z","shell.execute_reply":"2025-03-10T06:01:03.122324Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 98/98 [03:20<00:00,  2.04s/it]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## 8. Evaluate Predictions\n- Compares predicted scores against actual labels using QWK, Spearman’s Rank Correlation, and confusion matrix.","metadata":{}},{"cell_type":"code","source":"y_true = X_test[\"score\"].tolist()\nevaluate(y_true, predicted_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T06:01:06.678987Z","iopub.execute_input":"2025-03-10T06:01:06.679301Z","iopub.status.idle":"2025-03-10T06:01:07.118271Z","shell.execute_reply.started":"2025-03-10T06:01:06.679274Z","shell.execute_reply":"2025-03-10T06:01:07.117526Z"}},"outputs":[{"name":"stdout","text":"Quadratic Weighted Kappa: 0.922\nSpearman's Rank Correlation: 0.918\nAccuracy: 0.612\nSoft Accuracy: 0.969\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}